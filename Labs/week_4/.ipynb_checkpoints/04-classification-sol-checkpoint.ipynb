{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "**COMP4670/8600 &mdash; Statistical Machine Learning &mdash; Week 4**\n",
    "\n",
    "In this lab we will build, train, and test a logistic regression classifier.\n",
    "\n",
    "### Assumed knowledge:\n",
    "\n",
    "- Optimisation in Python (week 1 lab)\n",
    "- Regression (week 2 lab)\n",
    "- Binary classification with logistic regression (week 3 lectures)\n",
    "\n",
    "### After this lab, you should be comfortable with:\n",
    "\n",
    "- Implementing logistic regression\n",
    "- Practical binary classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "We will be working with the census-income dataset, which shows income levels for people in the 1994 US Census. We will predict whether a person has $\\leq \\$50000$ or $> \\$50000$ income per year.\n",
    "\n",
    "The data are included with this notebook as `04-dataset.tsv`, a textfile where in each row of data, the individual entries are delimited by tab characters. Download the data from the [course website](https://machlearn.gitlab.io/sml2019/tutorials/04-dataset.tsv)\n",
    "Load the data into a NumPy array called `data` using `numpy.genfromtxt`:\n",
    "\n",
    "```python\n",
    "    numpy.genfromtxt(filename)\n",
    "```\n",
    "\n",
    "The column names are given in the variable `columns` below.\n",
    "The `income` column are the targets, and the other columns will form our data used to try and guess the `income`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['income', 'age', 'education', 'private-work', 'married', 'capital-gain', 'capital-loss', 'hours-per-week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(\"04-dataset.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap - Binary classification\n",
    "\n",
    "The idea behind this lab is that for each person, we want to\n",
    "try and predict if their income is above the threshold of $\\$50,000$ or not,\n",
    "based on a series of other data about their person: `age, education,...`.\n",
    "\n",
    "As per usual, for the $n^\\text{th}$ row, the first entry is the target $t_n$, and the rest\n",
    "forms the data vector $\\mathbf{x}_n$\n",
    "\n",
    "We have two classes, $C_1$ representing the class of $ <\\$ 50,000$, which corresponds to\n",
    "a target of $t_n = 0$, and $C_2$, representing the class of $ >\\$50,000$, corresponding to\n",
    "a target of $t_n = 1$. Our objective is to learn a discriminative function $f_{\\mathbf{w}}(\\mathbf{x})$,\n",
    "parametrised by a weight vector $\\mathbf{w}$ that\n",
    "predicts which income class the person is in, based on the data given.\n",
    "\n",
    "We assume that each piece of information $(t_n, \\mathbf{x}_n)$ is i.i.d, and\n",
    "that there is some hidden probability distribution from which these target/data points are drawn.\n",
    "We will construct a likelihood function that indicates ``what is the likelihood that of this particular\n",
    "weight vector $\\mathbf{w}$, given that we have observed the training data $(t_n, \\mathbf{x}_n)$?''.\n",
    "$\\{t_1, t_2, \\ldots \\}$ would be generated?\"\n",
    "\n",
    "## Recap - Feature map, basis function\n",
    "\n",
    "Now some classes are not linearly seperable (we cannot draw a line such that all of one class is on one side,\n",
    "and all of the other class is on the other side). But by applying a fixed non-linear \n",
    "transformation to the inputs $\\mathbf{x}_i$ first, the result is usually linearly, for a suitable choice\n",
    "of transformation $\\phi$. (See week 3, pg 342 of the lecture slides).\n",
    "\n",
    "We let\n",
    "$$\n",
    "\\mathbf{\\phi}_n := \\phi(\\mathbf{x}_n)\n",
    "$$\n",
    "\n",
    "The result is linear in $\\phi$ but not in $\\mathbf{x}$, so we work in feature space rather than\n",
    "input space.\n",
    "For the case of two classes, we could guess that the target is a linear combination of the features,\n",
    "$$\n",
    "\\hat{t}_n = \\mathbf{w}^T \\mathbf{\\phi}_n\n",
    "$$\n",
    "but $\\mathbf{w}^T \\mathbf{\\phi}_n$ is a real number, and we want $\\hat{t}_n \\in \\{0,1\\}$.\n",
    "We could threshold the result,\n",
    "$$\n",
    "\\hat{t}_n =\n",
    "\\begin{cases}\n",
    "1 & \\mathbf{w}^T \\mathbf{\\phi}_n \\geq 0 \\\\\n",
    "0 & \\mathbf{w}^T \\mathbf{\\phi}_n < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "but the discontinuity makes it impossible to define a sensible gradient. \n",
    "\n",
    "## Recap - Logistic Regression\n",
    "\n",
    "(We assume that the classes are already linearly seperable, and use our input space as our feature space.\n",
    "We also assume the data is i.i.d).\n",
    "\n",
    "Instead of using a hard threshold like above, in logistic regression\n",
    "we can use the sigmoid function $\\sigma(a)$\n",
    "$$\n",
    "\\sigma(a) := \\frac{1}{1 + e^{-a}}\n",
    "$$\n",
    "which has the intended effect of \"squishing\" the real line to the interval $[0,1]$.\n",
    "This gives a smooth version of the threshold function above, that we can differentiate.\n",
    "The numbers it returns can be interpreted as a probability of the estimated target $\\hat{t}$ belonging\n",
    "to a class $C_i$ given the element $\\phi$ of feature space. In the case of two classes, we define\n",
    "\\begin{align}\n",
    "p(C_1 | \\phi ) &:= \\sigma (\\mathbf{w}^T \\phi) \\\\\n",
    "p(C_2 | \\phi ) &:= 1 - p(C_1 | \\phi)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "The likelihood function $p(\\mathbf{t} | \\mathbf{w}, \\mathbf{x})$ is what we want to maximise as a function\n",
    "of $\\mathbf{w}$. Since $\\mathbf{x}$ is fixed, we usually write the likelihood function as $p(\\mathbf{t} | \\mathbf{w})$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{t} | \\mathbf{w})\n",
    "&= \\prod_{i=1}^n p(t_i | \\mathbf{w}) \\\\\n",
    "&= \\prod_{i=1}^n \n",
    "\\begin{cases}\n",
    "p(C_1 | \\phi_i) & t_i = 1 \\\\\n",
    "p(C_2 | \\phi_i) & t_i = 0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "Note that\n",
    "$$\n",
    "\\begin{cases}\n",
    " y_n & t_i = 1 \\\\\n",
    "1 - y_n & t_i = 0\n",
    "\\end{cases}\n",
    "= y_n^{t_n} (1-y_n)^{1-t-n}\n",
    "$$\n",
    "as if $t_n = 1$, then $y_n^1 (1-y_n)^{1-1} = y_n$ and if $t_n = 0$ then $y_n^0 (1-y_n)^{1-0} = 1-y_n$.\n",
    "This is why we use the strange encoding of $t_i=0$ corresponds to $C_2$ and $t_i=1$ corresponds to $C_1$.\n",
    "Hence, our likelihood function is \n",
    "$$\n",
    "p(\\mathbf{t} | \\mathbf{w}) = \\prod_{i=1}^N y_n^{t_n} (1-y_n)^{1-t-n}, \\quad y_n = \\sigma(\\mathbf{w}^T \\phi_n)\n",
    "$$\n",
    "This function is quite unpleasant to try and differentiate, but we note that $p(\\mathbf{t} | \\mathbf{w})$\n",
    "is maximised when $\\log p(\\mathbf{t} | \\mathbf{w})$ is maximised.\n",
    "\\begin{align}\n",
    "\\log p(\\mathbf{t} | \\mathbf{w}) \n",
    "&= \\log \\prod_{i=1}^N y_n^{t_n} (1-y_n)^{1-t-n} \\\\\n",
    "&= \\sum_{i=1}^N \\log \\left( y_n^{t_n} (1-y_n)^{1-t-n} \\right) \\\\\n",
    "&= \\sum_{i=1}^N \\left( t_n \\log y_n +  (1-t_n) \\log (1-y_n) \\right)\n",
    "\\end{align}\n",
    "Which is maximised when $- \\log p(\\mathbf{t} | \\mathbf{w})$ is minimised, giving us our error function.\n",
    "$$\n",
    "E(\\mathbf{w}) := - \\sum_{i=1}^N \\left( t_n \\log y_n +  (1-t_n) \\log (1-y_n) \\right)\n",
    "$$\n",
    "We can then take the derivative of this. As an exercise, you should do this (using the identity $\\sigma'(a) = \\sigma(a) \\left( 1- \\sigma(a) \\right)$ to simplify).\n",
    "$$\n",
    "\\nabla_\\mathbf{w} E(\\mathbf{w}) = \\sum_{i=1}^N (y_n - t_n) \\phi_n\n",
    "$$\n",
    "which you will note doesn't have any sigmoid functions.\n",
    "\n",
    "(We also usually divide the error by the number of data points, to obtain the average error. The error\n",
    "shouldn't get 10 times as large just because there is more data avaliable, so we should divide by the\n",
    "number of error points to reflect that.)\n",
    "\n",
    "## Recap - $L_2$ regularisation, Gaussian prior\n",
    "\n",
    "To help avoid overfitting, we can add a penalty term to the cost function of the form \n",
    "$\\frac{\\lambda}{2} ||\\mathbf{w}||^2$. By tweaking the value of $\\lambda$, we can indicate how\n",
    "much to penalise large terms in the weight vector $\\mathbf{w}$. Don't forget to take the regularistion term into\n",
    "account when you compute the corresponding gradient $\\nabla_\\mathbf{w} E(\\mathbf{w})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain logistic regression (10 minutes)\n",
    "\n",
    "Find a partner in your lab (or groups of 3). Take turns to explain the topics above to each other, without referring to the lab sheet. Be as precise as possible, by writing down the relevant equations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with logistic regression\n",
    "\n",
    "Implement binary classification using logistic regression and $L_2$ regularisation. Make sure you write good quality code with comments and docstrings where appropriate.\n",
    "\n",
    "Use ```scipy.optimize.fmin_bfgs``` to optimise your cost function. ```fmin_bfgs``` takes as arguments the cost function to be optimised, and a tuple of extra arguments to the cost function:\n",
    "\n",
    "```python\n",
    "    scipy.optimise.fmin_bfgs(cost_function, initial_guess, args=())\n",
    "```\n",
    "\n",
    "By following equations in lectures, implement three functions:\n",
    "\n",
    "- `grad(w, X, t, a)`, which calculates the gradient of the cost function,\n",
    "- `train(X, t, a)`, which returns the maximum likelihood weight vector, and\n",
    "- `test(w, X)`, which returns predicted class probabilities,\n",
    "\n",
    "where \n",
    "* $w$ is a weight vector, \n",
    "* $X$ is a matrix of examples, \n",
    "* $t$ is a vector of labels/targets, \n",
    "* $a$ is the regularisation weight. \n",
    "\n",
    "(We would use $\\lambda$ for the regularisation term, but `a` is easier to type than `lambda`, and\n",
    "`lambda` is a reserved keyword in python, for lambda functions).\n",
    "\n",
    "See below for expected usage.\n",
    "\n",
    "We add an extra column of ones to represent the bias term.\n",
    "\n",
    "## Note\n",
    "\n",
    "* You should use 80% of the data as your training set, and 20% of the data as your test set.\n",
    "* You also may want to normalise the data before hand. If the magnitude of $\\mathbf{w}^T \\phi_n$\n",
    "is very large, the gradient of $\\sigma(\\mathbf{w}^T \\phi_n)$ will be very near zero, which can\n",
    "cause convergence issues during numerical minimisation. If each element in a particular column is\n",
    "multiplied by a scalar (say, all elements of the `age` column) then the result is essentially the same\n",
    "as stretching the space in which the data lives. The model will also be proportionally stretched,\n",
    "but will not fundamentally change the behaviour. So by normalising each column, we can avoid\n",
    "issues related to numerical convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert data.shape[1] == 8, 'you already ran this cell!'\n",
    "data = np.concatenate([data, np.ones((data.shape[0], 1))], axis=1)  # add a column of ones\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1 / (1 + np.exp(-a))\n",
    "\n",
    "def train(X, t, a):\n",
    "    fprime = grad\n",
    "    return opt.fmin_bfgs(\n",
    "        f=cost, fprime=fprime, x0=np.random.normal(scale=0.2, size=(X.shape[1],)), args=(X, t, a), disp=0)\n",
    "\n",
    "def test(w, X):\n",
    "    return sigmoid(X @ w)\n",
    "\n",
    "def cost(w, X, t, a):\n",
    "    # binary cross-entropy loss is\n",
    "    # t log y + (1 - t) log (1 - y)\n",
    "    y = test(w, X)\n",
    "    \n",
    "    mask = (y > 0) & (y < 1)\n",
    "    \n",
    "    t = t[mask]\n",
    "    y = y[mask]\n",
    "   # print(\"Mask is : \", y)\n",
    "    # regularisation L2 penalty\n",
    "    l2 = (w**2).sum() * a / 2\n",
    "    likelihood = t @ np.log(y) + (1 - t) @ np.log(1 - y)\n",
    "    cost = l2 - likelihood\n",
    "    return cost\n",
    "\n",
    "##SELF SOLUTION ###\n",
    "###################\n",
    "\n",
    "def grad(w, X, t, a):\n",
    "    y = test(w, X)\n",
    "  #  print(\"Shape of X and y is : \",np.shape(X) , \" y is : \",np.shape(y) ,\"t is : \", np.shape(t))\n",
    "    print(\"Shape of the gradient is : \", (X * (y - t)[:, None]).sum(axis=0) + a * w)\n",
    "    return (X * (y - t)[:, None]).sum(axis=0) + a * w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution \n",
    "\n",
    "N = np.shape(data)[0]\n",
    "num_train = int(0.8*N)\n",
    "y_train = data[:num_train,0]\n",
    "y_test = data[num_train:, 0]\n",
    "#32561\n",
    "norm_data = data[:, 1:] / data[:, 1:].sum(axis=0, keepdims=True)\n",
    "X_train = norm_data[:num_train, 1:]\n",
    "X_test = norm_data[num_train:, 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # solution\n",
    "\n",
    "# # triangulate the implementation of grad() and cost():\n",
    "\n",
    "# def numerical_grad(f, x, *args):\n",
    "#     eps = 1e-5\n",
    "#     fx = f(x, *args)\n",
    "#     grad = np.zeros_like(x)\n",
    "#     for i in range(len(x)):\n",
    "#         xi = x.copy()\n",
    "#         xi[i] += eps\n",
    "#         fxi = f(xi, *args)\n",
    "#         grad[i] = (fxi-fx) / eps\n",
    "#     return grad\n",
    "\n",
    "# w = np.random.randn(X_train.shape[1])\n",
    "# for a in (0, 99):\n",
    "#     ga = grad(w, X_train, y_train, a)\n",
    "#     gn = numerical_grad(cost, w, X_train, y_train, a)\n",
    "#     plt.figure()\n",
    "#     plt.plot(ga, gn, '.')\n",
    "#     plt.xlabel('analytical gradient')\n",
    "#     plt.ylabel('numeritcal gradient')\n",
    "#     plt.title('a=%f' % a)\n",
    "#     assert np.allclose(ga, gn, rtol=1e-2, atol=1e-2), list(zip(ga, gn))\n",
    "# del a, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the gradient is :  [ 0.18358124  0.22948462  0.03958026 -0.34315567 -0.04263766  0.18304035\n",
      "  0.24217572]\n",
      "Shape of the gradient is :  [ 0.16522054  0.20653221  0.03562039 -0.30874211 -0.03837359  0.16473356\n",
      "  0.21795515]\n",
      "Shape of the gradient is :  [ 0.12606401  0.15758346  0.02717578 -0.23537297 -0.02927986  0.12569205\n",
      "  0.16630162]\n",
      "Shape of the gradient is :  [ 0.03465378  0.04331392  0.0074628  -0.06415495 -0.0080505   0.03455043\n",
      "  0.04571709]\n",
      "Shape of the gradient is :  [ 1.43096701e-04  1.73615005e-04  2.14691869e-05  4.10489123e-04\n",
      " -3.54052940e-05  1.41315201e-04  1.91729512e-04]\n",
      "Shape of the gradient is :  [ 1.28665815e-04  1.56104303e-04  1.92895578e-05  3.69525162e-04\n",
      " -3.18376174e-05  1.27062892e-04  1.72399472e-04]\n",
      "Shape of the gradient is :  [ 9.81084325e-05  1.19033014e-04  1.46901244e-05  2.81770065e-04\n",
      " -2.42794964e-05  9.68857283e-05  1.31462949e-04]\n",
      "Shape of the gradient is :  [ 2.67731484e-05  3.24904232e-05  3.95848311e-06  7.69068423e-05\n",
      " -6.63433234e-06  2.64381962e-05  3.58956258e-05]\n",
      "Shape of the gradient is :  [-6.22107263e-09  1.23291771e-09 -6.32642633e-08 -9.85913740e-10\n",
      " -9.12490028e-09 -7.73322734e-09  1.67285875e-08]\n",
      "[-1.79389052 -2.25881091 -0.51415342  3.17457225  0.26087628 -1.84469169\n",
      " -2.0828591 ]\n"
     ]
    }
   ],
   "source": [
    "# solution\n",
    "\n",
    "w_train = train(X_train, y_train, 1e-1) #tweak regularisation parameter so weights are not big\n",
    "# y_guess = test(w_train, X_test)\n",
    "# print(y_guess)\n",
    "print(w_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance measure\n",
    "\n",
    "There are many ways to compute the performance of a binary classifier. The key concept is the idea of a confusion matrix:\n",
    "\n",
    "|     &nbsp;         | &nbsp;  | Label | &nbsp;  |\n",
    "|:-------------:|:--:|:-----:|:--:|\n",
    "|     &nbsp;         |  &nbsp;  |  0    | 1  |\n",
    "|**Prediction**| 0  |    TN | FN |\n",
    "|      &nbsp;        | 1  |    FP | TP |\n",
    "\n",
    "where\n",
    "* TP - true positive\n",
    "* FP - false positive\n",
    "* FN - false negative\n",
    "* TN - true negative\n",
    "\n",
    "Implement three functions:\n",
    "\n",
    "- `confusion_matrix(y_true, y_pred)`, which returns the confusion matrix as a list of lists given a list of true labels and a list of predicted labels;\n",
    "- `accuracy(cm)`, which takes a confusion matrix and returns the accuracy; and\n",
    "- `balanced_accuracy(cm)`, which takes a confusion matrix and returns the balanced accuracy.\n",
    "\n",
    "The accuracy is defined as $\\frac{TP + TN}{n}$, where $n$ is the total number of examples. The balanced accuracy is defined as $\\frac{1}{2}\\left(\\frac{TP}{P} + \\frac{TN}{N}\\right)$, where $T$ and $N$ are the total number of positive and negative examples respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    y_pred = y_pred > 0.5\n",
    "    y_true = y_true > 0.5\n",
    "    tp = ((y_true == y_pred) & (y_true)).sum()\n",
    "    tn = ((y_true == y_pred) & (~y_true)).sum()\n",
    "    fp = ((y_true != y_pred) & (~y_true)).sum()\n",
    "    fn = ((y_true != y_pred) & (y_true)).sum()\n",
    "    return [[tn, fn], [fp, tp]]\n",
    "\n",
    "def accuracy(cm):\n",
    "    (TN, FN), (FP, TP) = cm\n",
    "    return (TN + TP) / (TN + FN + FP + TP)\n",
    "\n",
    "def balanced_accuracy(cm):\n",
    "    (TN, FN), (FP, TP) = cm\n",
    "    return (TN / (TN + FP) + TP / (TP + FN)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy vs balanced accuracy\n",
    "\n",
    "What is the purpose of balanced accuracy? When might you prefer it to accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution\n",
    "Balanced accuracy accounts for class imbalance. In the extreme case, you would use it if you have imbalanced classes.\n",
    "For example, suppose you had some model that you were trying to learn if a patient had cancer based on\n",
    "MRI scan data, and the prior probability on any one person having cancer is 0.1%.\n",
    "Out of thousands of training examples, only a handful might actually have cancer. By just making the model always\n",
    "say \"no, this person doesn't have cancer\" it's already 99.9% accurate. During training, the model may just \n",
    "naturally do this, by overfitting to the cases where cancer was not present.\n",
    "But the penalty for a false positive \n",
    "(the model says you have cancer when you don't) is far less bad than a false negative (the model says you are okay when you do have cancer). So we need our accuracy to be weighted by the number of training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Consider the following code, which trains on all the examples, predicts on the training set, and then computes the accuracy and balanced accuracy. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7859665284814985, 0.7555762338020402]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "\n",
    "cmatrix = confusion_matrix(y_guess, y_test)\n",
    "[accuracy(cmatrix), balanced_accuracy(cmatrix)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution\n",
    "We get higher accuracy than balanced accuracy. This means that our data are class-imbalanced, and logistic regression may be over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking back at the prediction task\n",
    "\n",
    "Based on your results, what feature of the dataset is most useful for determining the income level? What feature is least useful? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['income', 'age', 'education', 'private-work', 'married', 'capital-gain', 'capital-loss', 'hours-per-week'] [-1.79389043 -2.25881083 -0.51415332  3.17457224  0.26087606 -1.84469165\n",
      " -2.08285926]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# solution\n",
    "plt.bar(range(len(w_train)), abs(w_train))\n",
    "plt.ylabel('importance')\n",
    "plt.xlabel('feature')\n",
    "plt.xticks(range(len(w_train)), columns[1:] + ['bias'], rotation='vertical');\n",
    "print(columns,w_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution\n",
    "\n",
    "The most important feature is **married** because it has the highest weight. (we ignore the bias term, as it just helps to shift the logistic regressor to help seperate the two classes. It doesn't actually correspond to any input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
